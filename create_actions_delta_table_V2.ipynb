{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f95a481-fc5a-499e-9a6c-c999a49d71e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Creates Delta Table for YinzCam Realtime API : Actions, Sessions, Hardware, Geo IP Join\n",
    "\n",
    "## Description\n",
    "This notebook generates a delta table for \"actions_sessions_geo_hardware\" to be reused later in the mc pipeline.\n",
    "This process saves execution time as this table won't have to recalculated for each notebook.\n",
    "\n",
    "## Process\n",
    "\n",
    "1. Ingest Actions, Sessions, GeoIp, Hardware from ADL\n",
    "2. Join Actions & Sessions & GeoIp & Hardware tables together\n",
    "3. Write to a delta table\n",
    "\n",
    "### Clean up Notes\n",
    "\n",
    "* spark.sql(\"DELETE FROM delta. `{}`\".format(delta_url))\n",
    "* spark.sql(\"VACUUM delta. `{}` RETAIN 168 HOURS\".format(delta_url))\n",
    "* dbutils.fs.rm(delta_url,recurse=True)\n",
    "\n",
    "## Outputs\n",
    "Delta Tables\n",
    "* `actions_sessions_geo_hardware`\n",
    "\n",
    "## QA\n",
    "\n",
    "* Nicole Ridout, Data Engineer, Nicole.Ridout@MLSE.com\n",
    "* Farah Bastien, Manager of Data Science, Farah.Bastien@MLSE.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4931bb45-1cec-417e-9199-f52422669351",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load the necessary functions from `PySpark` and `Python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1371f52d-ea6f-4f01-80f7-1698bd9e314c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SQL-like functions from PySpark\n",
    "from pyspark.sql.functions import col,date_format,from_utc_timestamp, unix_timestamp, sum, count,countDistinct, month,min,max,when,collect_set\n",
    "from pyspark.sql.functions import least, greatest, size, isnan, explode, lit, broadcast, concat\n",
    "from pyspark.sql.types import StructType, StructField, BinaryType, BooleanType, ByteType, DateType, DecimalType, DoubleType, FloatType, IntegerType,\\\n",
    "           LongType, ShortType, StringType, TimestampType\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#python packages\n",
    "from time import time, sleep, mktime\n",
    "from datetime import datetime, date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9845ebd5-479c-4ad8-a963-7b99c1a48e8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load ADL credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e691a88-a347-4789-bae4-8b9a773bdb28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = \"https://login.microsoftonline.com/{0}/oauth2/token\".format(dbutils.secrets.get(scope = \"adl_cred\", key = \"directory_id\"))\n",
    "spark.conf.set(\"dfs.adls.oauth2.access.token.provider.type\", \"ClientCredential\")\n",
    "spark.conf.set(\"dfs.adls.oauth2.client.id\", dbutils.secrets.get(scope = \"adl_cred\", key = \"client_id\"))\n",
    "spark.conf.set(\"dfs.adls.oauth2.credential\", dbutils.secrets.get(scope = \"adl_cred\", key = \"credential\"))\n",
    "spark.conf.set(\"dfs.adls.oauth2.refresh.url\", url)\n",
    "# spark.conf.set(\"spark.sql.broadcastTimeout\", \"36000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04cf0f04-3794-4e79-8c68-54a48663d33c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Define the team and url from ADL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6208d3c9-c1b7-478c-875a-093d0499a6ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Working with nhl team\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {
        "team": {
         "defaultValue": "",
         "label": "",
         "name": "team",
         "options": {
          "autoCreated": null,
          "validationRegex": null,
          "widgetType": "text"
         },
         "widgetType": "text"
        }
       },
       "arguments": {
        "team": "nhl"
       },
       "data": "<div class=\"ansiout\">Working with nhl team\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.widgets.text(\"team\", \"\",\"\")\n",
    "dbutils.widgets.get(\"team\")\n",
    "team = getArgument(\"team\")\n",
    "print(\"Working with {0} team\".format(team))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94ba6351-672c-4eb6-9ab2-917e7f5ec752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">adl://mlse1.azuredatalakestore.net/yinz_cam/nhl_tor/realtime_api/\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">adl://mlse1.azuredatalakestore.net/yinz_cam/nhl_tor/realtime_api/\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "adlurl = \"adl://mlse1.azuredatalakestore.net/yinz_cam/\"+ team +\"_tor/realtime_api/\"\n",
    "print(adlurl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bb91dd4-b871-42ea-b8a4-9bb4b1c4078a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load Realtime API Data from ADL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2a6b558-5dcf-40c3-913f-a628a1858680",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n",
       "<span class=\"ansigreen\">&lt;command-3794624804331560&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n",
       "<span class=\"ansigreen\">      1</span> timeFmt <span class=\"ansiyellow\">=</span> <span class=\"ansiblue\">&quot;yyyy-MM-ddTHH:mm:ss.SSS&quot;</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">      2</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">----&gt; 3</span><span class=\"ansiyellow\"> actions  = (spark.read.csv(adlurl + &apos;actions&apos;,header=True)\n",
       "</span><span class=\"ansigreen\">      4</span>             <span class=\"ansiyellow\">.</span>drop_duplicates<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">      5</span>             <span class=\"ansiyellow\">.</span>withColumnRenamed<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;id&apos;</span><span class=\"ansiyellow\">,</span><span class=\"ansiblue\">&apos;action_id&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansicyan\">csv</span><span class=\"ansiblue\">(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)</span>\n",
       "<span class=\"ansigreen\">    474</span>             path <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>path<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    475</span>         <span class=\"ansigreen\">if</span> type<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">==</span> list<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">--&gt; 476</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_df<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jreader<span class=\"ansiyellow\">.</span>csv<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_spark<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonUtils<span class=\"ansiyellow\">.</span>toSeq<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    477</span>         <span class=\"ansigreen\">elif</span> isinstance<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">,</span> RDD<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    478</span>             <span class=\"ansigreen\">def</span> func<span class=\"ansiyellow\">(</span>iterator<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n",
       "<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n",
       "<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n",
       "</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n",
       "<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n",
       "\n",
       "<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n",
       "<span class=\"ansigreen\">    326</span>                 raise Py4JJavaError(\n",
       "<span class=\"ansigreen\">    327</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">--&gt; 328</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n",
       "</span><span class=\"ansigreen\">    329</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n",
       "<span class=\"ansigreen\">    330</span>                 raise Py4JError(\n",
       "\n",
       "<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o353.csv.\n",
       ": com.microsoft.azure.datalake.store.ADLException: Error getting info for file /yinz_cam/nhl_tor/realtime_api/actions\n",
       "Operation GETFILESTATUS failed with HTTP500 : null\n",
       "Last encountered exception thrown after 5 tries. [HTTP500(null),HTTP500(null),HTTP500(null),HTTP500(null),HTTP500(null)]\n",
       " [ServerRequestId:8660ea99-d0d6-46da-95fe-4db1bfb96128]\n",
       "\tat com.microsoft.azure.datalake.store.ADLStoreClient.getExceptionFromResponse(ADLStoreClient.java:1169)\n",
       "\tat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:737)\n",
       "\tat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:718)\n",
       "\tat com.databricks.adl.AdlFileSystem.getFileStatus(AdlFileSystem.java:451)\n",
       "\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:612)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:596)\n",
       "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
       "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
       "\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
       "\tat scala.collection.immutable.List.flatMap(List.scala:355)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:596)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:391)\n",
       "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:307)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:293)\n",
       "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:717)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:295)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-3794624804331560&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> timeFmt <span class=\"ansiyellow\">=</span> <span class=\"ansiblue\">&quot;yyyy-MM-ddTHH:mm:ss.SSS&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 3</span><span class=\"ansiyellow\"> actions  = (spark.read.csv(adlurl + &apos;actions&apos;,header=True)\n</span><span class=\"ansigreen\">      4</span>             <span class=\"ansiyellow\">.</span>drop_duplicates<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span>             <span class=\"ansiyellow\">.</span>withColumnRenamed<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;id&apos;</span><span class=\"ansiyellow\">,</span><span class=\"ansiblue\">&apos;action_id&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansicyan\">csv</span><span class=\"ansiblue\">(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)</span>\n<span class=\"ansigreen\">    474</span>             path <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>path<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    475</span>         <span class=\"ansigreen\">if</span> type<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">==</span> list<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 476</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_df<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jreader<span class=\"ansiyellow\">.</span>csv<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_spark<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonUtils<span class=\"ansiyellow\">.</span>toSeq<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    477</span>         <span class=\"ansigreen\">elif</span> isinstance<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">,</span> RDD<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    478</span>             <span class=\"ansigreen\">def</span> func<span class=\"ansiyellow\">(</span>iterator<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1255</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1256</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1257</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1258</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1259</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    327</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 328</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    329</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    330</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o353.csv.\n: com.microsoft.azure.datalake.store.ADLException: Error getting info for file /yinz_cam/nhl_tor/realtime_api/actions\nOperation GETFILESTATUS failed with HTTP500 : null\nLast encountered exception thrown after 5 tries. [HTTP500(null),HTTP500(null),HTTP500(null),HTTP500(null),HTTP500(null)]\n [ServerRequestId:8660ea99-d0d6-46da-95fe-4db1bfb96128]\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getExceptionFromResponse(ADLStoreClient.java:1169)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:737)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:718)\n\tat com.databricks.adl.AdlFileSystem.getFileStatus(AdlFileSystem.java:451)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:612)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:596)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:596)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:391)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:293)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:717)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\n</div>",
       "errorSummary": "com.microsoft.azure.datalake.store.ADLException: Error getting info for file /yinz_cam/nhl_tor/realtime_api/actions",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "timeFmt = \"yyyy-MM-ddTHH:mm:ss.SSS\"\n",
    "\n",
    "actions  = (spark.read.csv(adlurl + 'actions',header=True)\n",
    "            .drop_duplicates()\n",
    "            .withColumnRenamed('id','action_id')\n",
    "            .withColumn('request_date_time',from_utc_timestamp(col('request_date_time').cast(TimestampType()), \"America/Toronto\"))\n",
    "            .withColumn('invisible_date_time',from_utc_timestamp(col('invisible_date_time').cast(TimestampType()), \"America/Toronto\"))\n",
    "            .withColumn('action_date',date_format('request_date_time', 'yyyy-MM-dd'))\n",
    "            .withColumn('dwell_time_s',unix_timestamp(\"invisible_date_time\",timeFmt) - unix_timestamp(\"request_date_time\",timeFmt))\n",
    "           )\n",
    "\n",
    "sessions = (spark.read.csv(adlurl + 'sessions',header=True)\n",
    "            .withColumnRenamed('id','ses_id')\n",
    "            .withColumn('start_date_time', from_utc_timestamp(col('start_date_time').cast(TimestampType()), \"America/Toronto\"))\n",
    "            .withColumn('end_date_time', from_utc_timestamp(col('end_date_time').cast(TimestampType()), \"America/Toronto\"))\n",
    "            .withColumn('session_date',date_format('start_date_time', 'yyyy-MM-dd'))\n",
    "            .orderBy('end_date_time',ascending=False)\n",
    "            .drop_duplicates(subset=['ses_id'])\n",
    "            .where(col('ses_id').isNotNull())\n",
    "            .withColumn(\"hardware_device_id\",col(\"hardware_device_id\").cast(IntegerType()))\n",
    "           )\n",
    "\n",
    "hardware = (spark.read.csv(adlurl + 'hardware',header=True)\n",
    "            .withColumnRenamed('id','hardware_id')\n",
    "            .drop_duplicates(subset=['hardware_id'])\n",
    "            .where(col('hardware_id').isNotNull())\n",
    "            .withColumn(\"hardware_id\",col(\"hardware_id\").cast(IntegerType()))\n",
    "           )\n",
    "\n",
    "geoip    = (spark.read.csv(adlurl + 'geoip' ,header=True)\n",
    "            .withColumnRenamed('id','geoip_id')\n",
    "            .withColumn('geoip_id',col('geoip_id').cast(IntegerType()))\n",
    "            .where(col('geoip_id').isNotNull())\n",
    "           )\n",
    "\n",
    "geoip = (geoip\n",
    "         .orderBy(geoip.columns, ascending=[False for i in range(len(geoip.columns))])\n",
    "         .drop_duplicates(subset=['geoip_id'])\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58f4998b-aefc-4886-80a0-f4cc3572bcba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Actions Sessions Hardware Geo Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9317e808-5021-4a2e-952b-cd8b6e0a7344",
     "showTitle": false,
     "title": "Actions/Sessions/Hardware/Geo Join"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join Actions & Sessions\n",
    "actions_sessions = (actions.join(sessions,col('session_id') == col('device_generated_id')))\n",
    "\n",
    "# Join actions_sessions with Geo\n",
    "actions_sessions_geo = actions_sessions.join(geoip,col('session_device_generated_id') == col('device_generated_id'))\n",
    "\n",
    "# The hardware table is a very small table about 5000 records, and the other table is 300 million. So we use \"Broadcast Join\" to optimize the join on this situation\n",
    "actions_sessions_geo_hardware = broadcast(hardware).join(actions_sessions_geo,col('hardware_id')==col('hardware_device_id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ce25200-6bff-4d0f-8fda-f6008a228a75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Write Actions/Sessions/GeoIp/Hardware to Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "485eb018-dc0c-417a-856e-e2c13ee8e6dc",
     "showTitle": false,
     "title": "Clean old Actions Sessions Hardware Geo Delta Table data"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean old Actions Sessions Hardware Geo Delta Table data\n",
    "delta_url = '/delta/yinzcam/' + team + '/actions_sessions_hardware_geo'\n",
    "spark.sql(\"VACUUM delta. `{}` RETAIN 168 HOURS\".format(delta_url))\n",
    "spark.sql(\"CLEAR CACHE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c9d620f-265b-4cba-a2c6-b0696e93c06d",
     "showTitle": false,
     "title": "Convert Actions Sessions Geo Hardware Join to Delta Table"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "actions_sessions_geo_hardware.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/yinzcam/\" + team + \"/actions_sessions_hardware_geo\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "create_actions_delta_table_V2",
   "widgets": {
    "team": {
     "currentValue": "nhl",
     "nuid": "9339aabd-1f61-40b7-87cf-96eff247d705",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "team",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
